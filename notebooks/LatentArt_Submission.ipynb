{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project LatentArt: LatentArt Submission\n",
    "## Level: ADVANCED\n",
    "\n",
    "This notebook documents the implementation and experimentation for the Image Generation Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print('\ud83d\ude80 Initializing Environment...\\n')\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    import shutil\n",
    "    REPO_NAME = 'LatentArt'\n",
    "    \n",
    "    try:\n",
    "        token = userdata.get('GH_TOKEN')\n",
    "        username = 'urobin84'\n",
    "        repo_url = f'https://{token}@github.com/{username}/{REPO_NAME}.git'\n",
    "        \n",
    "        os.chdir('/content')\n",
    "        if os.path.exists(REPO_NAME): shutil.rmtree(REPO_NAME)\n",
    "        !git clone {repo_url}\n",
    "        \n",
    "        root_path = f'/content/{REPO_NAME}'\n",
    "        if root_path not in sys.path: sys.path.insert(0, root_path)\n",
    "        os.chdir(root_path)\n",
    "        !pip install -r requirements.txt\n",
    "        print('\\n\u2705 Colab Setup Complete!')\n",
    "    except Exception as e:\n",
    "        print(f'\u274c Colab Setup Failed: {e}')\n",
    "else:\n",
    "    # LOCAL JUPYTER LOGIC\n",
    "    current_dir = os.getcwd()\n",
    "    if os.path.basename(current_dir) == 'notebooks':\n",
    "        root_path = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "    else:\n",
    "        root_path = current_dir\n",
    "        \n",
    "    if root_path not in sys.path: sys.path.insert(0, root_path)\n",
    "    print(f'\ud83c\udfe0 Running locally. Root detected: {root_path}')\n",
    "    print('\u2705 Local environment search path updated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in cast\")\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, EulerAncestralDiscreteScheduler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gunakan utilitas bawaan, jika ini berjalan di repositori GitHub kita\n",
    "try:\n",
    "    from src.utils import get_device, clear_memory\n",
    "    device = get_device()\n",
    "    print(f'\u2705 Kernel Ready. Device detected: {device}')\n",
    "except ImportError as e:\n",
    "    print(f'\u26a0\ufe0f Package failed loading: {e}')\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_scheduler(pipeline, scheduler_name='Euler A'):\n",
    "    if scheduler_name == 'Euler A':\n",
    "        pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "    print(f'Scheduler set to: {scheduler_name}')\n",
    "\n",
    "negative_prompt = 'photorealistic, realistic, photograph, 3d render, messy, blurry, low quality, bad art, ugly, sketch, grainy, unfinished, chromatic aberration'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2b07\ufe0f Download / Load Pipeline & Save Format to GitHub Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "model_path = os.path.join(root_path, \"LatentArt-Model\")\n",
    "\n",
    "# Jika model pernah didownload ke folder GitHub, muat dari repositori lokal!\n",
    "# Jika tidak, download dari cloud HuggingFace kemudian simpan.\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"\ud83d\udce6 Memuat (Load) Model secara lokal dari folder Github: {model_path}!\")\n",
    "    pipe_t2i = StableDiffusionPipeline.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "else:\n",
    "    print(f\"\u2601\ufe0f Mendownload Model dari HuggingFace (Pastikan koneksi internet aktif)...\")\n",
    "    pipe_t2i = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "    print(f\"\ud83d\udd27 Menyimpan model ke direktori repositori Github ({model_path}) agar bisa ditarik tanpa download ulang nantinya...\")\n",
    "    try:\n",
    "        pipe_t2i.save_pretrained(model_path)\n",
    "        print(f\"\ud83d\udcbe Model berhasil di-save!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Gagal menyimpan model: {e}\")\n",
    "\n",
    "# Memory optimizations untuk Apple Silicon M1/M2\n",
    "if device == \"mps\":\n",
    "    pipe_t2i.enable_attention_slicing()\n",
    "    pipe_t2i.enable_vae_slicing()\n",
    "    pipe_t2i.enable_vae_tiling()\n",
    "    pipe_t2i.enable_model_cpu_offload()\n",
    "elif device == \"cuda\":\n",
    "    pipe_t2i = pipe_t2i.to(\"cuda\")\n",
    "    pipe_t2i.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "load_scheduler(pipe_t2i, \"Euler A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kriteria 1: Base Text-to-Image Generation (Menggunakan Local Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A futuristic city in the style of cyberpunk, vivid neon lights, high detail, trending on artstation\"\n",
    "seed = 222\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "image = pipe_t2i(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7.5,\n",
    "    generator=generator\n",
    ").images[0]\n",
    "\n",
    "output_dir = os.path.join(root_path, \"outputs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "image.save(os.path.join(output_dir, \"basic_t2i.png\"))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kriteria 2: Image-To-Image Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meminjam struktur T2I untuk Refiner (Image-to-Image) tanpa mendownload/memuat RAM dua kali\n",
    "pipe_img2img = StableDiffusionImg2ImgPipeline(**pipe_t2i.components)\n",
    "\n",
    "# Memory Protection Sebelum Refinement!\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "refined_image = pipe_img2img(\n",
    "    prompt=prompt + \", masterpiece, 8k resolution, ultra detailed\",\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image.resize((768, 768)), # Upscaling Base Image to Refiner Dimension\n",
    "    strength=0.75,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=8.0,\n",
    "    generator=torch.Generator(device=device).manual_seed(111)\n",
    ").images[0]\n",
    "\n",
    "refined_image.save(os.path.join(output_dir, \"advanced_refined.png\"))\n",
    "refined_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}